{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "import os\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from math import ceil\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "# Model configuration\n",
    "img_width, img_height = 28, 28\n",
    "batch_size = 250\n",
    "no_epochs = 20\n",
    "no_classes = 10\n",
    "validation_split = 0.2\n",
    "verbosity = 1\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "\n",
    "# Load MNIST dataset\n",
    "(input_train, target_train), (input_test, target_test) = mnist.load_data()\n",
    "\n",
    "# Reshape data\n",
    "input_train = input_train.reshape(input_train.shape[0], img_width, img_height, 1)\n",
    "input_test = input_test.reshape(input_test.shape[0], img_width, img_height, 1)\n",
    "input_shape = (img_width, img_height, 1)\n",
    "    \n",
    "# Prepare model model saving directory.\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'cifar10_%s_model.{epoch:03d}.h5'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "\n",
    "# Parse numbers as floats\n",
    "input_train = input_train.astype('float32')\n",
    "input_test = input_test.astype('float32')\n",
    "\n",
    "# Normalize data\n",
    "input_train = input_train / 255\n",
    "input_test = input_test / 255\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(no_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s 88us/step - loss: 0.3545 - acc: 0.8943 - val_loss: 0.0956 - val_acc: 0.9712\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0949 - acc: 0.9707 - val_loss: 0.0662 - val_acc: 0.9817\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0708 - acc: 0.9780 - val_loss: 0.0589 - val_acc: 0.9836\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0543 - acc: 0.9825 - val_loss: 0.0500 - val_acc: 0.9861\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0488 - acc: 0.9849 - val_loss: 0.0365 - val_acc: 0.9887\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0392 - acc: 0.9876 - val_loss: 0.0343 - val_acc: 0.9902\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0342 - acc: 0.9887 - val_loss: 0.0334 - val_acc: 0.9903\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0314 - acc: 0.9895 - val_loss: 0.0362 - val_acc: 0.9892\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0252 - acc: 0.9919 - val_loss: 0.0319 - val_acc: 0.9906\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0233 - acc: 0.9926 - val_loss: 0.0397 - val_acc: 0.9891\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0219 - acc: 0.9929 - val_loss: 0.0335 - val_acc: 0.9910\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0195 - acc: 0.9933 - val_loss: 0.0280 - val_acc: 0.9918\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0177 - acc: 0.9945 - val_loss: 0.0297 - val_acc: 0.9917\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0169 - acc: 0.9943 - val_loss: 0.0281 - val_acc: 0.9923\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0155 - acc: 0.9948 - val_loss: 0.0275 - val_acc: 0.9918\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0143 - acc: 0.9952 - val_loss: 0.0343 - val_acc: 0.9912\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0122 - acc: 0.9959 - val_loss: 0.0276 - val_acc: 0.9929\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0124 - acc: 0.9958 - val_loss: 0.0279 - val_acc: 0.9920\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0109 - acc: 0.9960 - val_loss: 0.0357 - val_acc: 0.9909\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0108 - acc: 0.9963 - val_loss: 0.0304 - val_acc: 0.9919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1abe855c710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Fit data to model\n",
    "model.fit(input_train, target_train, batch_size=batch_size,\n",
    "          epochs=no_epochs,\n",
    "          verbose=verbosity,\n",
    "          validation_split=validation_split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Width & Height Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 6.393764922332764 / Test accuracy: 0.175\n"
     ]
    }
   ],
   "source": [
    "# specify the width an height shift arguments\n",
    "width_shift_val = 0.5\n",
    "height_shift_val = 0.5\n",
    "\n",
    "# create the class object\n",
    "datagen = ImageDataGenerator(width_shift_range=width_shift_val, height_shift_range=height_shift_val)\n",
    "\n",
    "# fit the generator\n",
    "datagen.fit(input_test.reshape(input_test.shape[0], 28, 28, 1))\n",
    "\n",
    "# Generate generalization metrics\n",
    "score = model.evaluate(datagen.flow(input_test, target_test), verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 3.4169360160827638 / Test accuracy: 0.5704\n"
     ]
    }
   ],
   "source": [
    "# specify the maximum rotation_range angle\n",
    "rotation_range_val = 90\n",
    "\n",
    "# create the class object\n",
    "datagen = ImageDataGenerator(rotation_range=rotation_range_val)\n",
    "\n",
    "# fit the generator\n",
    "datagen.fit(input_test.reshape(input_test.shape[0], 28, 28, 1))\n",
    "\n",
    "# Generate generalization metrics\n",
    "score = model.evaluate(datagen.flow(input_test, target_test), verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.3220121685028077 / Test accuracy: 0.6248\n"
     ]
    }
   ],
   "source": [
    "# specify the shear argument\n",
    "shear_range_val=85\n",
    "\n",
    "# create the class object\n",
    "datagen = ImageDataGenerator(shear_range=shear_range_val)\n",
    "\n",
    "# fit the generator\n",
    "datagen.fit(input_test.reshape(input_test.shape[0], 28, 28, 1))\n",
    "\n",
    "# Generate generalization metrics\n",
    "score = model.evaluate(datagen.flow(input_test, target_test), verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.5636787059783934 / Test accuracy: 0.2356\n"
     ]
    }
   ],
   "source": [
    "# specify the zoom argument\n",
    "zoom_range_val=[2.5,3.5]\n",
    "\n",
    "# create the class object\n",
    "datagen = ImageDataGenerator(zoom_range=zoom_range_val)\n",
    "\n",
    "# fit the generator\n",
    "datagen.fit(input_test.reshape(input_test.shape[0], 28, 28, 1))\n",
    "\n",
    "# Generate generalization metrics\n",
    "score = model.evaluate(datagen.flow(input_test, target_test), verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotate + W&H Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 4.5558368072509765 / Test accuracy: 0.3907\n"
     ]
    }
   ],
   "source": [
    "# specify the maximum rotation_range angle\n",
    "rotation_range_val = 60\n",
    "\n",
    "# specify the width and height shift arguments\n",
    "width_shift_val = 0.2\n",
    "height_shift_val = 0.2\n",
    "\n",
    "# create the class object\n",
    "datagen = ImageDataGenerator(rotation_range=rotation_range_val, width_shift_range=width_shift_val, height_shift_range=height_shift_val)\n",
    "\n",
    "# fit the generator\n",
    "datagen.fit(input_test.reshape(input_test.shape[0], 28, 28, 1))\n",
    "\n",
    "# Generate generalization metrics\n",
    "score = model.evaluate(datagen.flow(input_test, target_test), verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotate + W&H Shift + Shear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 5.234360201263428 / Test accuracy: 0.3404\n"
     ]
    }
   ],
   "source": [
    "# specify the maximum rotation_range angle\n",
    "rotation_range_val = 60\n",
    "\n",
    "# specify the shear argument\n",
    "shear_range_val = 45\n",
    "\n",
    "# specify the width and height shift arguments\n",
    "width_shift_val = 0.2\n",
    "height_shift_val = 0.2\n",
    "\n",
    "# create the class object\n",
    "datagen = ImageDataGenerator(rotation_range=rotation_range_val, width_shift_range=width_shift_val, height_shift_range=height_shift_val, shear_range=shear_range_val)\n",
    "\n",
    "# fit the generator\n",
    "datagen.fit(input_test.reshape(input_test.shape[0], 28, 28, 1))\n",
    "\n",
    "# Generate generalization metrics\n",
    "score = model.evaluate(datagen.flow(input_test, target_test), verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotate + W&H Shift + Shear + Zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 3.5423620849609376 / Test accuracy: 0.1799\n"
     ]
    }
   ],
   "source": [
    "# specify the maximum rotation_range angle\n",
    "rotation_range_val = 60\n",
    "\n",
    "# specify the shear argument\n",
    "shear_range_val = 45\n",
    "\n",
    "# specify the width and height shift arguments\n",
    "width_shift_val = 0.2\n",
    "height_shift_val = 0.2\n",
    "\n",
    "# specify the zoom argument\n",
    "zoom_range_val=[2.5,3.5]\n",
    "\n",
    "# create the class object\n",
    "datagen = ImageDataGenerator(rotation_range=rotation_range_val, width_shift_range=width_shift_val, height_shift_range=height_shift_val, shear_range=shear_range_val, zoom_range=zoom_range_val)\n",
    "\n",
    "# fit the generator\n",
    "datagen.fit(input_test.reshape(input_test.shape[0], 28, 28, 1))\n",
    "\n",
    "# Generate generalization metrics\n",
    "score = model.evaluate(datagen.flow(input_test, target_test), verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
