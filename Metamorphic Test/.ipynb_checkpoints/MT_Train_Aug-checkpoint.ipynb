{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "import os\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from math import ceil\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "# Model configuration\n",
    "img_width, img_height = 28, 28\n",
    "batch_size = 250\n",
    "no_epochs = 20\n",
    "no_classes = 10\n",
    "validation_split = 0.2\n",
    "verbosity = 1\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "\n",
    "# Load MNIST dataset\n",
    "(input_train, target_train), (input_test, target_test) = mnist.load_data()\n",
    "\n",
    "# Reshape data\n",
    "input_train = input_train.reshape(input_train.shape[0], img_width, img_height, 1)\n",
    "input_test = input_test.reshape(input_test.shape[0], img_width, img_height, 1)\n",
    "input_shape = (img_width, img_height, 1)\n",
    "    \n",
    "# Prepare model model saving directory.\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'cifar10_%s_model.{epoch:03d}.h5'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "\n",
    "# Parse numbers as floats\n",
    "input_train = input_train.astype('float32')\n",
    "input_test = input_test.astype('float32')\n",
    "\n",
    "# Normalize data\n",
    "input_train = input_train / 255\n",
    "input_test = input_test / 255\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(no_classes, activation='softmax'))\n",
    "\n",
    "# specify the width and height shift arguments\n",
    "width_shift_val = 0.5\n",
    "height_shift_val = 0.5\n",
    "\n",
    "# create the class object\n",
    "datagen = ImageDataGenerator(width_shift_range=width_shift_val, height_shift_range=height_shift_val)\n",
    "\n",
    "# fit the generator\n",
    "datagen.fit(input_train.reshape(input_train.shape[0], 28, 28, 1))\n",
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 1.6302 - acc: 0.4159\n",
      "Epoch 2/20\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.9960 - acc: 0.6540\n",
      "Epoch 3/20\n",
      "240/240 [==============================] - 8s 32ms/step - loss: 0.8300 - acc: 0.7122\n",
      "Epoch 4/20\n",
      "240/240 [==============================] - 7s 31ms/step - loss: 0.7393 - acc: 0.7446\n",
      "Epoch 5/20\n",
      "240/240 [==============================] - 8s 31ms/step - loss: 0.6921 - acc: 0.7605\n",
      "Epoch 6/20\n",
      "240/240 [==============================] - 7s 31ms/step - loss: 0.6439 - acc: 0.7771\n",
      "Epoch 7/20\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.6119 - acc: 0.7867\n",
      "Epoch 8/20\n",
      "240/240 [==============================] - 8s 32ms/step - loss: 0.5809 - acc: 0.7973\n",
      "Epoch 9/20\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.5656 - acc: 0.8036\n",
      "Epoch 10/20\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.5469 - acc: 0.8107\n",
      "Epoch 11/20\n",
      "240/240 [==============================] - 8s 31ms/step - loss: 0.5375 - acc: 0.8139\n",
      "Epoch 12/20\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.5186 - acc: 0.8179\n",
      "Epoch 13/20\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.5098 - acc: 0.8222\n",
      "Epoch 14/20\n",
      "240/240 [==============================] - 8s 31ms/step - loss: 0.5001 - acc: 0.8250\n",
      "Epoch 15/20\n",
      "240/240 [==============================] - 7s 31ms/step - loss: 0.4960 - acc: 0.8263\n",
      "Epoch 16/20\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.4883 - acc: 0.8274\n",
      "Epoch 17/20\n",
      "240/240 [==============================] - 7s 31ms/step - loss: 0.4801 - acc: 0.8315\n",
      "Epoch 18/20\n",
      "240/240 [==============================] - 7s 31ms/step - loss: 0.4739 - acc: 0.8338\n",
      "Epoch 19/20\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.4625 - acc: 0.8367\n",
      "Epoch 20/20\n",
      "240/240 [==============================] - 8s 31ms/step - loss: 0.4566 - acc: 0.8401\n",
      "Test loss: 0.0548952435759129 / Test accuracy: 0.9815\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fit data to model\n",
    "model.fit(datagen.flow(input_train, target_train, batch_size=batch_size),\n",
    "          epochs=no_epochs,\n",
    "          verbose=verbosity,\n",
    "          validation_split=validation_split)\n",
    "\n",
    "# Generate generalization metrics\n",
    "score = model.evaluate(input_test, target_test, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
